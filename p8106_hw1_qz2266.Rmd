---
title: "p8106_hw1_qz2266"
author: "Qing Zhou"
date: "2023-02-21"
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(dplyr) 
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


### Data Import and cleaning

In this exercise, we predict the sale price of a house using its other characteristics. The training data are in “housing train.csv”, and the test data are in “housing test.csv”.
```{r}
# read in training data
train = read.csv("data/housing_training.csv") %>% 
janitor::clean_names()
train = na.omit(train)

# read in test data
test = read.csv("data/housing_test.csv") %>% 
janitor::clean_names()
test = na.omit(test)

# create covariates matrix for training and test
x_train = model.matrix(sale_price ~ ., train)[,-1]
y_train = train$sale_price
x_test <- model.matrix(sale_price ~ ., test)[ ,-1]
y_test <- test$sale_price
```


Check for potential collinearities between predictors in training data
```{r correlation plot}
# Correlation plot for all predictors
corrplot(cor(x_train), method = "circle", type = "full", tl.cex = 0.5)
```
From the correlation plot we can see there are high correlations between some of the covariates. This high correlation might cause collinearity issue. To fix the potential multicollinearit issue, regularization methods such as lasso, elastic net, or partial least squares could be employed, other than linear model. 



### a). Linear model 
```{r}
set.seed(1)

lm_fit = lm(sale_price ~ ., 
            data = train,
            method = "lm",
            trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5))
summary(lm_fit)

pred_lm = predict(lm_fit, newdata = test)
lm_mse = RMSE(pred_lm, test$sale_price); lm_mse
```




